{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Federated Learning Under the Lens of Task Arithmetic\n",
        "\n",
        "This notebook runs all experiments for the project:\n",
        "1. Centralized baseline training\n",
        "2. FedAvg with IID/non-IID sharding\n",
        "3. Sparse fine-tuning with task arithmetic\n",
        "4. Mask strategy comparison (extension)\n",
        "\n",
        "**Important**: This notebook is designed for Google Colab with GPU runtime."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/VitoFe/amlproject\n",
        "%cd amlproject\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install uv and dependencies\n",
        "!pip install --upgrade uv\n",
        "!uv sync"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set checkpoint directory\n",
        "CHECKPOINT_DIR = './fl_checkpoints'\n",
        "LOG_DIR = './fl_logs'\n",
        "# Import project modules\n",
        "from src.data.dataset import get_cifar100_datasets, get_dataloaders\n",
        "from src.models.dino_vit import create_dino_vit\n",
        "from src.training.centralized import CentralizedTrainer\n",
        "from src.training.federated import FederatedTrainer\n",
        "from src.training.federated_sparse import FederatedSparseTrainer\n",
        "from src.utils.seed import set_seed\n",
        "from src.utils.logging import setup_logging\n",
        "from src.utils.visualization import plot_training_curves, plot_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment configuration\n",
        "CONFIG = {\n",
        "    'seed': 42,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'data_dir': './data',\n",
        "    'checkpoint_dir': CHECKPOINT_DIR,\n",
        "    'log_dir': LOG_DIR,\n",
        "    \n",
        "    # Data\n",
        "    'batch_size': 64,\n",
        "    'val_split': 0.1,\n",
        "    \n",
        "    # Centralized\n",
        "    'centralized_epochs': 50,\n",
        "    'centralized_lr': 0.001,\n",
        "    \n",
        "    # Federated\n",
        "    'num_clients': 100,       # K\n",
        "    'participation_rate': 0.1, # C\n",
        "    'local_steps': 4,          # J\n",
        "    'num_rounds': 500,\n",
        "    'federated_lr': 0.01,\n",
        "    \n",
        "    # Non-IID\n",
        "    'nc_values': [1, 5, 10, 50],\n",
        "    'j_values': [4, 8, 16],\n",
        "    \n",
        "    # Sparse\n",
        "    'sparsity_ratio': 0.9,\n",
        "    'calibration_rounds': 5,\n",
        "    'fisher_samples': 512,\n",
        "    \n",
        "    # Multiple runs\n",
        "    'num_runs': 3\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded\")\n",
        "print(f\"Device: {CONFIG['device']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load CIFAR-100\n",
        "set_seed(CONFIG['seed'])\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = get_cifar100_datasets(\n",
        "    data_dir=CONFIG['data_dir'],\n",
        "    val_split=CONFIG['val_split'],\n",
        "    seed=CONFIG['seed']\n",
        ")\n",
        "\n",
        "train_loader, val_loader, test_loader = get_dataloaders(\n",
        "    train_dataset, val_dataset, test_dataset,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} samples\")\n",
        "print(f\"Val: {len(val_dataset)} samples\")\n",
        "print(f\"Test: {len(test_dataset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 1: Centralized Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to delete old checkpoints and start fresh\n",
        "# import shutil, os\n",
        "# shutil.rmtree(CHECKPOINT_DIR, ignore_errors=True)\n",
        "# os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "\n",
        "# Create model with regularization\n",
        "model = create_dino_vit(\n",
        "    num_classes=100, \n",
        "    device=CONFIG['device'],\n",
        "    dropout=0.3,\n",
        "    freeze_layers=6\n",
        ")\n",
        "print(f\"Model parameters: {model.count_parameters()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train centralized baseline\n",
        "centralized_config = {\n",
        "    'epochs': 20,\n",
        "    'learning_rate': 0.0005,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 0.01,\n",
        "    'scheduler': 'cosine',\n",
        "    'label_smoothing': 0.1,\n",
        "    'early_stopping_patience': 10,\n",
        "    'checkpoint_dir': CHECKPOINT_DIR,\n",
        "    'log_dir': LOG_DIR\n",
        "}\n",
        "\n",
        "trainer = CentralizedTrainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    config=centralized_config,\n",
        "    device=CONFIG['device'],\n",
        "    experiment_name='centralized_regularized'\n",
        ")\n",
        "\n",
        "# (resume=False to start fresh, or True to continue)\n",
        "centralized_results = trainer.train(resume=True, save_every=5)\n",
        "print(f\"\\nCentralized Test Accuracy: {centralized_results['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "plot_training_curves(\n",
        "    trainer.get_metrics_history(),\n",
        "    title='Centralized Training',\n",
        "    save_path=f\"{CONFIG['log_dir']}/centralized_curves.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 2: FedAvg with IID Sharding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FedAvg with IID data distribution\n",
        "set_seed(CONFIG['seed'])\n",
        "model = create_dino_vit(num_classes=100, device=CONFIG['device'])\n",
        "\n",
        "federated_config = {\n",
        "    'num_clients': CONFIG['num_clients'],\n",
        "    'participation_rate': CONFIG['participation_rate'],\n",
        "    'local_steps': CONFIG['local_steps'],\n",
        "    'num_rounds': CONFIG['num_rounds'],\n",
        "    'learning_rate': CONFIG['federated_lr'],\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 1e-4,\n",
        "    'early_stopping_patience': 10,\n",
        "    'batch_size': CONFIG['batch_size'],\n",
        "    'checkpoint_dir': CONFIG['checkpoint_dir'],\n",
        "    'log_dir': CONFIG['log_dir'],\n",
        "    'seed': CONFIG['seed'],\n",
        "    'sharding': {'strategy': 'iid', 'nc': 100}\n",
        "}\n",
        "\n",
        "trainer = FederatedTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    config=federated_config,\n",
        "    device=CONFIG['device'],\n",
        "    experiment_name='fedavg_iid'\n",
        ")\n",
        "\n",
        "fedavg_iid_results = trainer.train(resume=True)\n",
        "print(f\"\\nFedAvg (IID) Test Accuracy: {fedavg_iid_results['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 3: FedAvg with Non-IID Sharding (Varying Nc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different levels of data heterogeneity\n",
        "noniid_results = {}\n",
        "\n",
        "for nc in CONFIG['nc_values']:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Testing Nc = {nc} classes per client\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    set_seed(CONFIG['seed'])\n",
        "    model = create_dino_vit(num_classes=100, device=CONFIG['device'])\n",
        "    \n",
        "    config = {\n",
        "        'num_clients': CONFIG['num_clients'],\n",
        "        'participation_rate': CONFIG['participation_rate'],\n",
        "        'local_steps': CONFIG['local_steps'],\n",
        "        'num_rounds': CONFIG['num_rounds'],\n",
        "        'learning_rate': CONFIG['federated_lr'],\n",
        "        'momentum': 0.9,\n",
        "        'weight_decay': 1e-6,\n",
        "        'early_stopping_patience': 10,\n",
        "        'batch_size': CONFIG['batch_size'],\n",
        "        'checkpoint_dir': CONFIG['checkpoint_dir'],\n",
        "        'log_dir': CONFIG['log_dir'],\n",
        "        'seed': CONFIG['seed'],\n",
        "        'sharding': {'strategy': 'non_iid', 'nc': nc}\n",
        "    }\n",
        "    \n",
        "    trainer = FederatedTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader,\n",
        "        config=config,\n",
        "        device=CONFIG['device'],\n",
        "        experiment_name=f'fedavg_noniid_nc{nc}'\n",
        "    )\n",
        "    \n",
        "    results = trainer.train(resume=True)\n",
        "    noniid_results[nc] = results['accuracy']\n",
        "    print(f\"Nc={nc}: Test Accuracy = {results['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nHeterogeneity Experiment Results:\")\n",
        "print(\"-\" * 30)\n",
        "for nc, acc in noniid_results.items():\n",
        "    print(f\"Nc = {nc}: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 4: Sparse Fine-tuning (Task Arithmetic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Federated sparse fine-tuning with least-sensitive masking\n",
        "set_seed(CONFIG['seed'])\n",
        "model = create_dino_vit(num_classes=100, device=CONFIG['device'])\n",
        "\n",
        "sparse_config = {\n",
        "    'num_clients': CONFIG['num_clients'],\n",
        "    'participation_rate': CONFIG['participation_rate'],\n",
        "    'local_steps': CONFIG['local_steps'],\n",
        "    'num_rounds': CONFIG['num_rounds'],\n",
        "    'learning_rate': CONFIG['federated_lr'],\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 1e-4,\n",
        "    'batch_size': CONFIG['batch_size'],\n",
        "    'checkpoint_dir': CONFIG['checkpoint_dir'],\n",
        "    'log_dir': CONFIG['log_dir'],\n",
        "    'seed': CONFIG['seed'],\n",
        "    'sharding': {'strategy': 'iid', 'nc': 100},\n",
        "    'sparse': {\n",
        "        'sparsity_ratio': CONFIG['sparsity_ratio'],\n",
        "        'calibration_rounds': CONFIG['calibration_rounds'],\n",
        "        'mask_strategy': 'least_sensitive',\n",
        "        'fisher_samples': CONFIG['fisher_samples']\n",
        "    }\n",
        "}\n",
        "\n",
        "trainer = FederatedSparseTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_loader=val_loader,\n",
        "    test_loader=test_loader,\n",
        "    config=sparse_config,\n",
        "    device=CONFIG['device'],\n",
        "    experiment_name='fedavg_sparse_least_sensitive'\n",
        ")\n",
        "\n",
        "sparse_results = trainer.train(resume=True, calibrate_masks=True)\n",
        "print(f\"\\nSparse Fine-tuning Test Accuracy: {sparse_results['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment 5 (Extension): Mask Strategy Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all mask strategies\n",
        "strategies = [\n",
        "    'least_sensitive',\n",
        "    'most_sensitive',\n",
        "    'lowest_magnitude',\n",
        "    'highest_magnitude',\n",
        "    'random'\n",
        "]\n",
        "\n",
        "strategy_results = {}\n",
        "\n",
        "for strategy in strategies:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Testing strategy: {strategy}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    \n",
        "    set_seed(CONFIG['seed'])\n",
        "    model = create_dino_vit(num_classes=100, device=CONFIG['device'])\n",
        "    \n",
        "    config = sparse_config.copy()\n",
        "    config['sparse'] = {\n",
        "        'sparsity_ratio': CONFIG['sparsity_ratio'],\n",
        "        'calibration_rounds': CONFIG['calibration_rounds'],\n",
        "        'mask_strategy': strategy,\n",
        "        'fisher_samples': CONFIG['fisher_samples']\n",
        "    }\n",
        "    \n",
        "    trainer = FederatedSparseTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        val_loader=val_loader,\n",
        "        test_loader=test_loader,\n",
        "        config=config,\n",
        "        device=CONFIG['device'],\n",
        "        experiment_name=f'fedavg_sparse_{strategy}'\n",
        "    )\n",
        "    \n",
        "    results = trainer.train(resume=True, calibrate_masks=True)\n",
        "    strategy_results[strategy] = results['accuracy']\n",
        "    print(f\"{strategy}: Test Accuracy = {results['accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nMask Strategy Comparison:\")\n",
        "print(\"=\"*60)\n",
        "for strategy, acc in sorted(strategy_results.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{strategy:25s}: {acc:.4f}\")\n",
        "\n",
        "plot_comparison(\n",
        "    {s: {'test_accuracy': a} for s, a in strategy_results.items()},\n",
        "    metric='test_accuracy',\n",
        "    title='Mask Strategy Comparison',\n",
        "    save_path=f\"{CONFIG['log_dir']}/strategy_comparison.png\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"EXPERIMENT SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n1. Centralized Baseline: {centralized_results['accuracy']:.4f}\")\n",
        "print(f\"\\n2. FedAvg (IID): {fedavg_iid_results['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n3. FedAvg (Non-IID):\")\n",
        "for nc, acc in noniid_results.items():\n",
        "    print(f\"   Nc={nc}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\n4. Sparse Fine-tuning: {sparse_results['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\n5. Mask Strategy Comparison:\")\n",
        "for strategy, acc in sorted(strategy_results.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"   {strategy}: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to JSON\n",
        "import json\n",
        "\n",
        "all_results = {\n",
        "    'centralized': centralized_results['accuracy'],\n",
        "    'fedavg_iid': fedavg_iid_results['accuracy'],\n",
        "    'fedavg_noniid': noniid_results,\n",
        "    'sparse': sparse_results['accuracy'],\n",
        "    'mask_strategies': strategy_results\n",
        "}\n",
        "\n",
        "with open(f\"{CONFIG['log_dir']}/all_results.json\", 'w') as f:\n",
        "    json.dump(all_results, f, indent=2)\n",
        "\n",
        "print(f\"Results saved to {CONFIG['log_dir']}/all_results.json\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
